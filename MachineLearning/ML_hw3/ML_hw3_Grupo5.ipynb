{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Puntaje Bancario: Aprobación de Crédito Mediante Redes Neuronales\n",
    "\n",
    "En esta ocasión se busca desarrollar un protocolo de pruebas que permita encontrar la mejor arquitectura de red neuronal completamente conectada. En esta ocasión debe utilizar la librería SciKit-Learn (sklearn.neural_network) para diseñar cada red. Además, veremos algunos conceptos de _feature engineering_ para analizar los datos a nuestra disposición.\n",
    "\n",
    "Debe completar las celdas vacías y seguir las instrucciones anotadas en el cuaderno.\n",
    "\n",
    "La fecha límite de entrega es el día **18 de octubre** y se realizará a través de Bloque Neón."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd   \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import itertools\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se tienen dos archivos:\n",
    "\n",
    "`application_record.csv`: posee información general (puede observar los nombres de las columnas a continuación) sobre cada usuario, definido a partir de una ID única.\n",
    "\n",
    "- ID: número de cliente\n",
    "- CODE_GENDER: género\n",
    "- FLAG_OWN_CAR:\tposee un automóvil\n",
    "- FLAG_OWN_REALTY: posee un inmueble\n",
    "- CNT_CHILDREN:\tcantidad de hijos\n",
    "- AMT_INCOME_TOTAL:\tingresos anuales\n",
    "- NAME_INCOME_TYPE: categoría de ingresos\n",
    "- NAME_EDUCATION_TYPE: nivel educativo\n",
    "- NAME_FAMILY_STATUS estado civil\n",
    "- NAME_HOUSING_TYPE: forma de vivienda (e.g. renta, apartamento propio, ...)\n",
    "- DAYS_BIRTH: fecha de nacimiento, en días hacia atrás desde la actualidad, -1 significa ayer\n",
    "\n",
    "- DAYS_EMPLOYED: tiempo de empleo, en días hacia atrás desde la actualidad, -1 significa ayer. Si es positivo, el usuario se encuentra desempleado.\n",
    "- FLAG_MOBIL: teléfono móvil\n",
    "- FLAG_WORK_PHONE: teléfono de trabajo\n",
    "- FLAG_PHONE: teléfono\n",
    "- FLAG_EMAIL: email\n",
    "- OCCUPATION_TYPE: ocupación\n",
    "- CNT_FAM_MEMBERS: tamaño de familia\n",
    "\n",
    "\n",
    "`credit_record.csv`:\n",
    "\n",
    "- ID: número de cliente\n",
    "- MONTHS_BALANCE: mes de registro\n",
    "- ESTADO:\n",
    "    - 0: 1-29 días atrasados\n",
    "    - 1: 30-59 días atrasados\n",
    "    - 2: 60-89 días atrasados\n",
    "    - 3: 90-119 días atrasados\n",
    "    - 4: 120-149 días atrasados\n",
    "    - 5: Atrasados o incobrables, cancelaciones durante más de 150 días\n",
    "    - C: cancelado ese mes X: sin préstamo durante el mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"creditCardScore/application_record.csv\", encoding = 'utf-8') \n",
    "record = pd.read_csv(\"creditCardScore/credit_record.csv\", encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.facecolor'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>NAME_INCOME_TYPE</th>\n",
       "      <th>NAME_EDUCATION_TYPE</th>\n",
       "      <th>NAME_FAMILY_STATUS</th>\n",
       "      <th>NAME_HOUSING_TYPE</th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>FLAG_MOBIL</th>\n",
       "      <th>FLAG_WORK_PHONE</th>\n",
       "      <th>FLAG_PHONE</th>\n",
       "      <th>FLAG_EMAIL</th>\n",
       "      <th>OCCUPATION_TYPE</th>\n",
       "      <th>CNT_FAM_MEMBERS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>438552</th>\n",
       "      <td>6840104</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>Pensioner</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Separated</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-22717</td>\n",
       "      <td>365243</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438553</th>\n",
       "      <td>6840222</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>103500.0</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Single / not married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-15939</td>\n",
       "      <td>-3007</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Laborers</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438554</th>\n",
       "      <td>6841878</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>Commercial associate</td>\n",
       "      <td>Higher education</td>\n",
       "      <td>Single / not married</td>\n",
       "      <td>With parents</td>\n",
       "      <td>-8169</td>\n",
       "      <td>-372</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Sales staff</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438555</th>\n",
       "      <td>6842765</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>Pensioner</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-21673</td>\n",
       "      <td>365243</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438556</th>\n",
       "      <td>6842885</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>121500.0</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-18858</td>\n",
       "      <td>-1201</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Sales staff</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID CODE_GENDER FLAG_OWN_CAR FLAG_OWN_REALTY  CNT_CHILDREN  \\\n",
       "438552  6840104           M            N               Y             0   \n",
       "438553  6840222           F            N               N             0   \n",
       "438554  6841878           F            N               N             0   \n",
       "438555  6842765           F            N               Y             0   \n",
       "438556  6842885           F            N               Y             0   \n",
       "\n",
       "        AMT_INCOME_TOTAL      NAME_INCOME_TYPE            NAME_EDUCATION_TYPE  \\\n",
       "438552          135000.0             Pensioner  Secondary / secondary special   \n",
       "438553          103500.0               Working  Secondary / secondary special   \n",
       "438554           54000.0  Commercial associate               Higher education   \n",
       "438555           72000.0             Pensioner  Secondary / secondary special   \n",
       "438556          121500.0               Working  Secondary / secondary special   \n",
       "\n",
       "          NAME_FAMILY_STATUS  NAME_HOUSING_TYPE  DAYS_BIRTH  DAYS_EMPLOYED  \\\n",
       "438552             Separated  House / apartment      -22717         365243   \n",
       "438553  Single / not married  House / apartment      -15939          -3007   \n",
       "438554  Single / not married       With parents       -8169           -372   \n",
       "438555               Married  House / apartment      -21673         365243   \n",
       "438556               Married  House / apartment      -18858          -1201   \n",
       "\n",
       "        FLAG_MOBIL  FLAG_WORK_PHONE  FLAG_PHONE  FLAG_EMAIL OCCUPATION_TYPE  \\\n",
       "438552           1                0           0           0             NaN   \n",
       "438553           1                0           0           0        Laborers   \n",
       "438554           1                1           0           0     Sales staff   \n",
       "438555           1                0           0           0             NaN   \n",
       "438556           1                0           1           0     Sales staff   \n",
       "\n",
       "        CNT_FAM_MEMBERS  \n",
       "438552              1.0  \n",
       "438553              1.0  \n",
       "438554              1.0  \n",
       "438555              2.0  \n",
       "438556              2.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>MONTHS_BALANCE</th>\n",
       "      <th>STATUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5001711</td>\n",
       "      <td>0</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5001711</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5001711</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5001711</td>\n",
       "      <td>-3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5001712</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID  MONTHS_BALANCE STATUS\n",
       "0  5001711               0      X\n",
       "1  5001711              -1      0\n",
       "2  5001711              -2      0\n",
       "3  5001711              -3      0\n",
       "4  5001712               0      C"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de los Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etiquetas\n",
    "\n",
    "Inicialmente, se concatenan ambas tablas mediante el tiempo de registro máximo (`MONTHS_BALANCE`) y la ID del cliente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all users' account open month.\n",
    "begin_month=pd.DataFrame(record.groupby([\"ID\"])[\"MONTHS_BALANCE\"].agg(min))\n",
    "begin_month=begin_month.rename(columns={'MONTHS_BALANCE':'begin_month'}) \n",
    "new_data=pd.merge(data,begin_month,how=\"left\",on=\"ID\") #merge to record data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los usuarios con mora durante más de 60 días se etiquerarán como `1`, de lo contrario, serán `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "record['dep_value'] = None\n",
    "record['dep_value'][record['STATUS'] =='2']='Yes' \n",
    "record['dep_value'][record['STATUS'] =='3']='Yes' \n",
    "record['dep_value'][record['STATUS'] =='4']='Yes' \n",
    "record['dep_value'][record['STATUS'] =='5']='Yes' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpunt=record.groupby('ID').count()\n",
    "cpunt['dep_value'][cpunt['dep_value'] > 0]='Yes' \n",
    "cpunt['dep_value'][cpunt['dep_value'] == 0]='No' \n",
    "cpunt = cpunt[['dep_value']]\n",
    "new_data=pd.merge(new_data,cpunt,how='inner',on='ID')\n",
    "new_data['target']=new_data['dep_value']\n",
    "new_data.loc[new_data['target']=='Yes','target']=1\n",
    "new_data.loc[new_data['target']=='No','target']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No     45318\n",
      "Yes      667\n",
      "Name: dep_value, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "No     0.985495\n",
       "Yes    0.014505\n",
       "Name: dep_value, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(cpunt['dep_value'].value_counts())\n",
    "cpunt['dep_value'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proporción de clases.\n",
    "\n",
    "### Descriptores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Renombramiento de las Columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.rename(columns={'CODE_GENDER':'Gender','FLAG_OWN_CAR':'Car','FLAG_OWN_REALTY':'Reality',\n",
    "                         'CNT_CHILDREN':'ChldNo','AMT_INCOME_TOTAL':'inc',\n",
    "                         'NAME_EDUCATION_TYPE':'edutp','NAME_FAMILY_STATUS':'famtp',\n",
    "                        'NAME_HOUSING_TYPE':'houtp','FLAG_EMAIL':'email',\n",
    "                         'NAME_INCOME_TYPE':'inctp','FLAG_WORK_PHONE':'wkphone',\n",
    "                         'FLAG_PHONE':'phone','CNT_FAM_MEMBERS':'famsize',\n",
    "                        'OCCUPATION_TYPE':'occyp'\n",
    "                        },inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.dropna()\n",
    "new_data = new_data.mask(new_data == 'NULL').dropna() # Retiramos los valores NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ivtable=pd.DataFrame(new_data.columns,columns=['variable'])\n",
    "ivtable['IV']=None\n",
    "namelist = ['FLAG_MOBIL','begin_month','dep_value','target','ID']\n",
    "\n",
    "for i in namelist:\n",
    "    ivtable.drop(ivtable[ivtable['variable'] == i].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones Auxiliares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se crean algunas funciones que serán utilizadas más adelante.\n",
    "\n",
    "Función `calc_iv` para obtener las variables IV (information value) y WoE (weight of evidence). Estas variables, de forma general, nos permiten conocer la importancia de cada feature disponible.\n",
    "\n",
    "Puede encontrar más información en:\n",
    "- https://www.kaggle.com/puremath86/iv-woe-starter-for-python\n",
    "- https://www.listendata.com/2015/03/weight-of-evidence-woe-and-information.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo de IV\n",
    "def calc_iv(df, feature, target, pr=False):\n",
    "    lst = []\n",
    "    df[feature] = df[feature].fillna(\"NULL\")\n",
    "\n",
    "    for i in range(df[feature].nunique()):\n",
    "        val = list(df[feature].unique())[i]\n",
    "        lst.append([feature,                                                        # Variable\n",
    "                    val,                                                            # Value\n",
    "                    df[df[feature] == val].count()[feature],                        # All\n",
    "                    df[(df[feature] == val) & (df[target] == 0)].count()[feature],  # Good (think: Fraud == 0)\n",
    "                    df[(df[feature] == val) & (df[target] == 1)].count()[feature]]) # Bad (think: Fraud == 1)\n",
    "\n",
    "    data = pd.DataFrame(lst, columns=['Variable', 'Value', 'All', 'Good', 'Bad'])\n",
    "    data['Share'] = data['All'] / data['All'].sum()\n",
    "    data['Bad Rate'] = data['Bad'] / data['All']\n",
    "    data['Distribution Good'] = (data['All'] - data['Bad']) / (data['All'].sum() - data['Bad'].sum())\n",
    "    data['Distribution Bad'] = data['Bad'] / data['Bad'].sum()\n",
    "    data['WoE'] = np.log(data['Distribution Good'] / data['Distribution Bad'])\n",
    "    \n",
    "    data = data.replace({'WoE': {np.inf: 0, -np.inf: 0}})\n",
    "\n",
    "    data['IV'] = data['WoE'] * (data['Distribution Good'] - data['Distribution Bad'])\n",
    "\n",
    "    data = data.sort_values(by=['Variable', 'Value'], ascending=[True, True])\n",
    "    data.index = range(len(data.index))\n",
    "\n",
    "    if pr:\n",
    "        print(data)\n",
    "        print('IV = ', data['IV'].sum())\n",
    "\n",
    "    iv = data['IV'].sum()\n",
    "    print('El IV de esta variable es:',iv)\n",
    "    print(df[feature].value_counts())\n",
    "    return iv, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificación One-Hot\n",
    "def convert_dummy(df, feature,rank=0):\n",
    "    pos = pd.get_dummies(df[feature], prefix=feature)\n",
    "    mode = df[feature].value_counts().index[rank]\n",
    "    biggest = feature + '_' + str(mode)\n",
    "    pos.drop([biggest],axis=1,inplace=True)\n",
    "    df.drop([feature],axis=1,inplace=True)\n",
    "    df=df.join(pos)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category(df, col, binsnum, labels, qcut = False):\n",
    "    if qcut:\n",
    "        localdf = pd.qcut(df[col], q = binsnum, labels = labels) # quantile cut\n",
    "    else:\n",
    "        localdf = pd.cut(df[col], bins = binsnum, labels = labels) # equal-length cut\n",
    "        \n",
    "    localdf = pd.DataFrame(localdf)\n",
    "    name = 'gp' + '_' + col\n",
    "    localdf[name] = localdf[col]\n",
    "    df = df.join(localdf[name])\n",
    "    df[name] = df[name].astype(object)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de Confusión\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Etiqueta')\n",
    "    plt.xlabel('Predicción')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptores Binarios\n",
    "\n",
    "Se utilizará la función desarrollada anteriormente para realizar un análisis de cada uno de los descriptores binarios y **su influencia dentro de la predicción de cada clase**.\n",
    "\n",
    "#### Género"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    15630\n",
      "1     9504\n",
      "Name: Gender, dtype: int64\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Value after * must be an iterable, not numpy.int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13908/3004270803.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnew_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Gender'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Gender'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'F'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'M'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Gender'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0miv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalc_iv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Gender'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mivtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mivtable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'variable'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'Gender'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'IV'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0miv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13908/539899126.py\u001b[0m in \u001b[0;36mcalc_iv\u001b[1;34m(df, feature, target, pr)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         lst.append([feature,                                                        # Variable\n\u001b[0m\u001b[0;32m      9\u001b[0m                     \u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m                  \u001b[1;33m*\u001b[0m\u001b[1;33m-\u001b[0m                                          \u001b[1;31m# Value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m                        \u001b[1;31m# All\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Value after * must be an iterable, not numpy.int64"
     ]
    }
   ],
   "source": [
    "new_data['Gender'] = new_data['Gender'].replace(['F','M'],[0,1])\n",
    "print(new_data['Gender'].value_counts())\n",
    "iv, data = calc_iv(new_data,'Gender','target')\n",
    "ivtable.loc[ivtable['variable']=='Gender','IV']=iv\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posesión de un Automóvil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['Car'] = new_data['Car'].replace(['N','Y'],[0,1])\n",
    "print(new_data['Car'].value_counts())\n",
    "iv, data=calc_iv(new_data,'Car','target')\n",
    "ivtable.loc[ivtable['variable']=='Car','IV']=iv\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posesión de un Inmueble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['Reality'] = new_data['Reality'].replace(['N','Y'],[0,1])\n",
    "print(new_data['Reality'].value_counts())\n",
    "iv, data=calc_iv(new_data,'Reality','target')\n",
    "ivtable.loc[ivtable['variable']=='Reality','IV']=iv\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posesión de un Teléfono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['phone']=new_data['phone'].astype(str)\n",
    "print(new_data['phone'].value_counts(normalize=True,sort=False))\n",
    "new_data.drop(new_data[new_data['phone'] == 'nan' ].index, inplace=True)\n",
    "iv, data=calc_iv(new_data,'phone','target')\n",
    "ivtable.loc[ivtable['variable']=='phone','IV']=iv\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posesión de Correo Electrónico (Email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_data['email'].value_counts(normalize=True,sort=False))\n",
    "new_data['email']=new_data['email'].astype(str)\n",
    "iv, data=calc_iv(new_data,'email','target')\n",
    "ivtable.loc[ivtable['variable']=='email','IV']=iv\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posesión de Teléfono para Trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['wkphone']=new_data['wkphone'].astype(str)\n",
    "iv, data = calc_iv(new_data,'wkphone','target')\n",
    "new_data.drop(new_data[new_data['wkphone'] == 'nan' ].index, inplace=True)\n",
    "ivtable.loc[ivtable['variable']=='wkphone','IV']=iv\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptores Continuos\n",
    "\n",
    "#### Cantidad de Hijos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos aquellos que: no tienen hijos, tienen 1 hijo, tienen 2 o más hijos.\n",
    "\n",
    "new_data.loc[new_data['ChldNo'] >= 2,'ChldNo']='2More'\n",
    "print(new_data['ChldNo'].value_counts(sort=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv, data=calc_iv(new_data,'ChldNo','target')\n",
    "ivtable.loc[ivtable['variable']=='ChldNo','IV']=iv\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = convert_dummy(new_data,'ChldNo') # Adicionamos una Codificación One-Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ingresos Anuales\n",
    "\n",
    "Gráfica de Histograma para observar la distribución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['inc'] = new_data['inc'].astype(object)\n",
    "new_data['inc'] = new_data['inc']/10000 \n",
    "print(new_data['inc'].value_counts(bins=10,sort=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['inc'].plot(kind='hist',bins=50,density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = get_category(new_data,'inc', 3, [\"low\",\"medium\", \"high\"], qcut = True)\n",
    "iv, data = calc_iv(new_data,'gp_inc','target')\n",
    "ivtable.loc[ivtable['variable']=='inc','IV']=iv\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = convert_dummy(new_data,'gp_inc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_data['Age']=-(new_data['DAYS_BIRTH'])//365\t\n",
    "print(new_data['Age'].value_counts(bins=10,normalize=True,sort=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['Age'].plot(kind='hist',bins=20,density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = get_category(new_data,'Age',5, [\"lowest\",\"low\",\"medium\",\"high\",\"highest\"])\n",
    "iv, data = calc_iv(new_data,'gp_Age','target')\n",
    "ivtable.loc[ivtable['variable']=='DAYS_BIRTH','IV'] = iv\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = convert_dummy(new_data,'gp_Age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Años de Trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['worktm']=-(new_data['DAYS_EMPLOYED'])//365\n",
    "new_data[new_data['worktm']<0] = np.nan # replace by na\n",
    "new_data['DAYS_EMPLOYED']\n",
    "new_data['worktm'].fillna(new_data['worktm'].mean(),inplace=True) #replace na by mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['worktm'].plot(kind='hist',bins=20,density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = get_category(new_data,'worktm',5, [\"lowest\",\"low\",\"medium\",\"high\",\"highest\"])\n",
    "iv, data=calc_iv(new_data,'gp_worktm','target')\n",
    "ivtable.loc[ivtable['variable']=='DAYS_EMPLOYED','IV']=iv\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificación One-Hot\n",
    "new_data = convert_dummy(new_data,'gp_worktm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tamaño de Familia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['famsize'].value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['famsize']=new_data['famsize'].astype(int)\n",
    "new_data['famsizegp']=new_data['famsize']\n",
    "new_data['famsizegp']=new_data['famsizegp'].astype(object)\n",
    "new_data.loc[new_data['famsizegp']>=3,'famsizegp']='3more'\n",
    "iv, data=calc_iv(new_data,'famsizegp','target')\n",
    "ivtable.loc[ivtable['variable']=='famsize','IV']=iv\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificación One-Hot\n",
    "new_data = convert_dummy(new_data,'famsizegp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptores Categóricos\n",
    "\n",
    "#### Forma de Ingresos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_data['inctp'].value_counts(sort=False))\n",
    "print(new_data['inctp'].value_counts(normalize=True,sort=False))\n",
    "new_data.loc[new_data['inctp']=='Pensioner','inctp']='State servant'\n",
    "new_data.loc[new_data['inctp']=='Student','inctp']='State servant'\n",
    "iv, data=calc_iv(new_data,'inctp','target')\n",
    "ivtable.loc[ivtable['variable']=='inctp','IV']=iv\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificación One-Hot\n",
    "new_data = convert_dummy(new_data,'inctp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tipo de Ocupación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.loc[(new_data['occyp']=='Cleaning staff') | (new_data['occyp']=='Cooking staff') | (new_data['occyp']=='Drivers') | (new_data['occyp']=='Laborers') | (new_data['occyp']=='Low-skill Laborers') | (new_data['occyp']=='Security staff') | (new_data['occyp']=='Waiters/barmen staff'),'occyp']='Laborwk'\n",
    "new_data.loc[(new_data['occyp']=='Accountants') | (new_data['occyp']=='Core staff') | (new_data['occyp']=='HR staff') | (new_data['occyp']=='Medicine staff') | (new_data['occyp']=='Private service staff') | (new_data['occyp']=='Realty agents') | (new_data['occyp']=='Sales staff') | (new_data['occyp']=='Secretaries'),'occyp']='officewk'\n",
    "new_data.loc[(new_data['occyp']=='Managers') | (new_data['occyp']=='High skill tech staff') | (new_data['occyp']=='IT staff'),'occyp']='hightecwk'\n",
    "print(new_data['occyp'].value_counts())\n",
    "iv, data=calc_iv(new_data,'occyp','target')\n",
    "ivtable.loc[ivtable['variable']=='occyp','IV']=iv\n",
    "data.head()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = convert_dummy(new_data,'occyp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forma de Vivienda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv, data=calc_iv(new_data,'houtp','target')\n",
    "ivtable.loc[ivtable['variable']=='houtp','IV']=iv\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = convert_dummy(new_data,'houtp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Educación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.loc[new_data['edutp']=='Academic degree','edutp']='Higher education'\n",
    "iv, data=calc_iv(new_data,'edutp','target')\n",
    "ivtable.loc[ivtable['variable']=='edutp','IV']=iv\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = convert_dummy(new_data,'edutp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Estado Civil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_data['famtp'].value_counts(normalize=True,sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv, data=calc_iv(new_data,'famtp','target')\n",
    "ivtable.loc[ivtable['variable']=='famtp','IV']=iv\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = convert_dummy(new_data,'famtp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilidad de: IV y WoE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede leer el artículo a continuación para comprender un poco más sobre los conceptos de IV y WoE:\n",
    "\n",
    "https://docs.tibco.com/pub/sfire-dsc/6.5.0/doc/html/TIB_sfire-dsc_user-guide/GUID-07A78308-525A-406F-8221-9281F4E9D7CF.html\n",
    "\n",
    "La tabla a continuación fue tomada de la referencia indicada:\n",
    "\n",
    "| IV| Ability to predict | \n",
    "|:------|:------:| \n",
    "| <0.02 | Bajo poder predictivo | \n",
    "|0.02~0.1 |Poder predictivo débil|\n",
    "|0.1~0.3|Poder predictivo moderado|\n",
    "|0.3~0.5|Poder predictivo fuerte|\n",
    "|>0.5|Sospechosamente alto, revisar esta variable| "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ivtable=ivtable.sort_values(by='IV',ascending=False)\n",
    "ivtable.loc[ivtable['variable']=='DAYS_BIRTH','variable']='agegp'\n",
    "ivtable.loc[ivtable['variable']=='DAYS_EMPLOYED','variable']='worktmgp'\n",
    "ivtable.loc[ivtable['variable']=='inc','variable']='incgp'\n",
    "ivtable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicción de Buen/Mal Cliente Mediante Redes Neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se tomarán únicamente aquellas columnas preprocesadas y con un $IV>0.001$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = new_data['target']\n",
    "X = new_data[['Gender','Reality','ChldNo_1', 'ChldNo_2More', 'gp_inc_medium',  'gp_inc_high','wkphone',\n",
    "              'gp_Age_high', 'gp_Age_highest', 'gp_Age_low',\n",
    "              'gp_Age_lowest','gp_worktm_high', 'gp_worktm_highest',\n",
    "              'gp_worktm_low', 'gp_worktm_medium','occyp_hightecwk', \n",
    "              'occyp_officewk','famsizegp_1', 'famsizegp_3more',\n",
    "              'houtp_Co-op apartment', 'houtp_Municipal apartment',\n",
    "              'houtp_Office apartment', 'houtp_Rented apartment',\n",
    "              'houtp_With parents','edutp_Higher education',\n",
    "              'edutp_Incomplete higher', 'edutp_Lower secondary','famtp_Civil marriage',\n",
    "              'famtp_Separated','famtp_Single / not married','famtp_Widow']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE\n",
    "\n",
    "Concepto: Synthetic Minority Over-Sampling Technique(`SMOTE`) utilizado para lidiar con datos desbalanceados. Puede encontrar más información en:\n",
    "\n",
    "- http://glemaitre.github.io/imbalanced-learn/generated/imblearn.over_sampling.SMOTE.html\n",
    "- https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y.astype('int')\n",
    "sm = SMOTE()\n",
    "X_balance,Y_balance = sm.fit_resample(X,Y)\n",
    "X_balance = pd.DataFrame(X_balance, columns = X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separación de datos en conjuntos: entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_balance,Y_balance, \n",
    "                                                    stratify=Y_balance, test_size=0.3,\n",
    "                                                    random_state = 10086)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\*Seleccione esta celda y luego la opción `Run All Above`\\*\n",
    "\n",
    "# PARTE 1\n",
    "\n",
    "## Separación de Conjunto de Features\n",
    "\n",
    "Teniendo en cuenta los resultados de IV obtenidos anteriormente, comprobaremos la capacidad predictiva de tres conjuntos de datos basados en la tabla anterior. Primero removeremos los últimos cuatro ('phone', 'inctp', 'email', 'Car'), y luego realizaremos la siguiente división:\n",
    "\n",
    "- A. Primera mitad: 'agegp', 'famtp', 'worktmgp', 'Reality', 'Gender', 'edutp'\n",
    "- B. Segunda mitad: 'houtp', 'famsize', 'occyp', 'incgp', 'wkphone', 'ChldNo'\n",
    "- C. Todos los descriptores.\n",
    "\n",
    "De acuerdo a estos nombres, utilice la siguiente lista para identificar aquellos solicitados en cada caso:\n",
    "```\n",
    "    'Gender','Reality','ChldNo_1', 'ChldNo_2More', 'gp_inc_medium',  'gp_inc_high','wkphone',\n",
    "    'gp_Age_high', 'gp_Age_highest', 'gp_Age_low',\n",
    "    'gp_Age_lowest','gp_worktm_high', 'gp_worktm_highest',\n",
    "    'gp_worktm_low', 'gp_worktm_medium','occyp_hightecwk', \n",
    "    'occyp_officewk','famsizegp_1', 'famsizegp_3more',\n",
    "    'houtp_Co-op apartment', 'houtp_Municipal apartment',\n",
    "    'houtp_Office apartment', 'houtp_Rented apartment',\n",
    "    'houtp_With parents','edutp_Higher education',\n",
    "    'edutp_Incomplete higher', 'edutp_Lower secondary','famtp_Civil marriage',\n",
    "    'famtp_Separated','famtp_Single / not married','famtp_Widow'\n",
    "```\n",
    "### A. Top 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se generan los nuevos conjuntos de entrenamiento y prueba para el caso A\n",
    "X_train_subA = X_train[['gp_Age_highest', 'gp_Age_highest', 'gp_Age_low', 'gp_Age_lowest', 'famtp_Civil marriage', 'famtp_Separated', 'famtp_Separated', 'famtp_Single / not married', 'famtp_Widow', 'gp_worktm_highest', 'gp_worktm_high', 'gp_worktm_medium', 'gp_worktm_low','Reality', 'Gender', 'edutp_Higher education', 'edutp_Incomplete higher', 'edutp_Lower secondary']]\n",
    "X_test_subA = X_test[['gp_Age_highest', 'gp_Age_highest', 'gp_Age_low', 'gp_Age_lowest', 'famtp_Civil marriage', 'famtp_Separated', 'famtp_Separated', 'famtp_Single / not married', 'famtp_Widow', 'gp_worktm_highest', 'gp_worktm_high', 'gp_worktm_medium', 'gp_worktm_low','Reality', 'Gender', 'edutp_Higher education', 'edutp_Incomplete higher', 'edutp_Lower secondary']]\n",
    "\n",
    "X_train_subA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Últimos 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Se generan los nuevos conjuntos de entrenamiento y prueba para el caso B\n",
    "X_train_subB = X_train[['houtp_Co-op apartment', 'houtp_Municipal apartment', 'houtp_With parents', 'famsizegp_1', 'famsizegp_3more', 'occyp_hightecwk', 'occyp_officewk', 'gp_inc_medium', 'gp_inc_high', 'wkphone', 'ChldNo_1', 'ChldNo_2More']]\n",
    "X_test_subB = X_test[['houtp_Co-op apartment', 'houtp_Municipal apartment', 'houtp_With parents', 'famsizegp_1', 'famsizegp_3more', 'occyp_hightecwk', 'occyp_officewk', 'gp_inc_medium', 'gp_inc_high', 'wkphone', 'ChldNo_1', 'ChldNo_2More']]\n",
    "\n",
    "X_train_subB.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso C\n",
    "\n",
    "Acá simplemente tomaremos, no es necesario crear nuevas variables:\n",
    "```\n",
    "X_train_subC = X_train\n",
    "X_test_subC = X_test\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se generan los nuevos conjuntos de entrenamiento y prueba para el caso C \n",
    "X_train_subC = X_train\n",
    "X_test_subC = X_test\n",
    "\n",
    "X_train_subC.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTE 2\n",
    "\n",
    "Implementación de pruebas en los conjuntos de descriptores. A continuación debe implementar, inicialmente tres modelos de regresión logística, y posteriormente tres redes neuronales (2 capas escondidas, 20 neuronas en cada una). Observe los resultados y analice lo sucedido. Concluya sobre qué modelo es deseable teniendo en cuenta la factibilidad de implementación práctica y la matriz de confusión correspondiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión Logística\n",
    "\n",
    "Inicialmente se probará un modelo de regresión logística para tener una referencia (también se conoce como _baseline_) y comprobar que un modelo de red neuronal permite obtener mejores resultados.\n",
    "\n",
    "$$\\log \\left({p \\over {1 - p}}\\right) = {\\beta _0} + {\\beta _1}{x_1} +  \\cdot  \\cdot  \\cdot  + {\\beta _q}{x_q}$$\n",
    "\n",
    "### Caso A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(C=0.8,\n",
    "                           random_state=0,\n",
    "                           solver='lbfgs')\n",
    "model.fit(X_train_subA, y_train) # Se ajusta el modelo con los datos del conjunto A #\n",
    "y_predict = model.predict(X_test_subA) # Se realiza la predicción de etiquetas con los datos de prueba del conjunto A #\n",
    "\n",
    "# Variable para guardar la precisión del caso A\n",
    "precLogCasoA = round(accuracy_score(y_test, y_predict),5)\n",
    "\n",
    "print(f'Precisión {precLogCasoA}\\n')\n",
    "print(pd.DataFrame(confusion_matrix(y_test,y_predict)))\n",
    "\n",
    "sns.set_style('white') \n",
    "class_names = ['0','1']\n",
    "plot_confusion_matrix(confusion_matrix(y_test,y_predict),\n",
    "                      classes= class_names, \n",
    "                      title='Matriz de Confusión Normalizada: Regresión Logística')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(C=0.8,\n",
    "                           random_state=0,\n",
    "                           solver='lbfgs')\n",
    "model.fit(X_train_subB, y_train) # Se ajusta el modelo con los datos del conjunto B #\n",
    "y_predict = model.predict(X_test_subB) # Se realiza la predicción de etiquetas con los datos de prueba del conjunto B #\n",
    "\n",
    "# Variable para guardar la precisión del caso B\n",
    "precLogCasoB = round(accuracy_score(y_test, y_predict),5)\n",
    "\n",
    "print(f'Precisión {precLogCasoB}\\n')\n",
    "print(pd.DataFrame(confusion_matrix(y_test,y_predict)))\n",
    "\n",
    "sns.set_style('white') \n",
    "class_names = ['0','1']\n",
    "plot_confusion_matrix(confusion_matrix(y_test,y_predict),\n",
    "                      classes= class_names, \n",
    "                      title='Matriz de Confusión Normalizada: Regresión Logística')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(C=0.8,\n",
    "                           random_state=0,\n",
    "                           solver='lbfgs')\n",
    "model.fit(X_train_subC, y_train) # Se ajusta el modelo con los datos del conjunto C #\n",
    "y_predict = model.predict(X_test_subC) # Se realiza la predicción de etiquetas con los datos de prueba del conjunto C #\n",
    "\n",
    "# Variable para guardar la precisión del caso C\n",
    "precLogCasoC = round(accuracy_score(y_test, y_predict),5)\n",
    "\n",
    "print(f'Precisión {precLogCasoC}\\n')\n",
    "print(pd.DataFrame(confusion_matrix(y_test,y_predict)))\n",
    "\n",
    "sns.set_style('white') \n",
    "class_names = ['0','1']\n",
    "plot_confusion_matrix(confusion_matrix(y_test,y_predict),\n",
    "                      classes= class_names, \n",
    "                      title='Matriz de Confusión Normalizada: Regresión Logística')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red Neuronal, Perceptrón Multicapa\n",
    "\n",
    "Ahora utilice la función `MLPClassifier` de la librería SciKit-Learn para desarrollar una red neuronal que permita mejorar el rendimiento del clasificador _baseline_ desarrollado.\n",
    "\n",
    "### Caso A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model = MLPClassifier(hidden_layer_sizes=(2, 20), activation='relu', random_state=1) # Se inicializa un modelo de red neuronal con 2 capas escondidas x 20 neuronas en cada capa y función de activación ReLu #\n",
    "model.fit(X_train_subA, y_train) # Se ajusta el modelo con los datos del conjunto A #\n",
    "y_predict = model.predict(X_test_subA) # Se realiza la predicción de etiquetas con los datos de prueba del conjunto A #\n",
    "\n",
    "# Variable para guardar la precisión del caso A (Neural Network)\n",
    "precNNCasoA = round(accuracy_score(y_test, y_predict),5)\n",
    "\n",
    "print(f'Precisión {precNNCasoA}')\n",
    "print(pd.DataFrame(confusion_matrix(y_test,y_predict)))\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix(y_test,y_predict),\n",
    "                      classes=class_names, \n",
    "                      title='Matriz de Confusión Normalizada: Red Neuronal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caso B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model = MLPClassifier(hidden_layer_sizes=(2, 20), activation='relu', random_state=1) # Se inicializa un modelo de red neuronal con 2 capas escondidas x 20 neuronas en cada capa y función de activación ReLu #\n",
    "model.fit(X_train_subB, y_train) # Se ajusta el modelo con los datos del conjunto B #\n",
    "y_predict = model.predict(X_test_subB) # Se realiza la predicción de etiquetas con los datos de prueba del conjunto B #\n",
    "\n",
    "# Variable para guardar la precisión del caso B (Neural Network)\n",
    "precNNCasoB = round(accuracy_score(y_test, y_predict),5)\n",
    "\n",
    "print(f'Precisión {precNNCasoB}')\n",
    "print(pd.DataFrame(confusion_matrix(y_test,y_predict)))\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix(y_test,y_predict),\n",
    "                      classes=class_names, \n",
    "                      title='Matriz de Confusión Normalizada: Red Neuronal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caso C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model = MLPClassifier(hidden_layer_sizes=(2, 20), activation='relu', random_state=1) # Se inicializa un modelo de red neuronal con 2 capas escondidas x 20 neuronas en cada capa y función de activación ReLu #\n",
    "model.fit(X_train_subC, y_train) # Se ajusta el modelo con los datos del conjunto C #\n",
    "y_predict = model.predict(X_test_subC) # Se realiza la predicción de etiquetas con los datos de prueba del conjunto C #\n",
    "\n",
    "# Variable para guardar la precisión del caso C (Neural Network)\n",
    "precNNCasoC = round(accuracy_score(y_test, y_predict),5)\n",
    "\n",
    "print(f'Precisión {precNNCasoC}')\n",
    "print(pd.DataFrame(confusion_matrix(y_test,y_predict)))\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix(y_test,y_predict),\n",
    "                      classes=class_names, \n",
    "                      title='Matriz de Confusión Normalizada: Red Neuronal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se genera una tabla con las precisiones de cada caso para poder ver la información resumida de manera organizada.\n",
    "precisiones = [[precLogCasoA, precLogCasoB, precLogCasoC, precNNCasoA, precNNCasoB, precNNCasoC]]\n",
    "dfPrecisiones = pd.DataFrame(precisiones, columns=['Precisión LogReg Caso A', 'Precisión LogReg Caso B', 'Precisión LogReg Caso C', 'Precisión NN Caso A', 'Precisión NN Caso B', 'Precisión NN Caso C'])\n",
    "\n",
    "dfPrecisiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones Parte 2\n",
    "\n",
    "Se pueden hacer las siguientes observaciones:\n",
    "\n",
    "- Para los tres casos de separación de datos, el entrenamiento del modelo de **regresión logística** entregó en todos los casos una *menor* precisión, lo que es de esperarse, pues este modelo de regresión no es tan robusto como una red neuronal. De hecho, se puede ver la regresión logística como una red neuronal de una capa con una sola neurona de activación sigmodial. Comparando esto con una red neuronal de dos capas y 20 neuronas en cada capa, es claro que, aunque la red neuronal de 2 capas y 20 neuronas por capa toma más tiempo para entrenar, entrega una mejor precisión porque se hace un mayor aprendizaje al tener varias neuronas y varias capas.\n",
    "\n",
    "- Mirando solo los tres casos entrenados con un modelo de **regresión logística**, se puede notar que el Caso C, en donde se entrenó el modelo usando todos los descriptores de las clases, fue el que entregó la mejor precisión de los tres, y esto puede ser debido al hecho de que, teniendo en cuenta todas las características del modelo se tiene un mejor criterio para el clasificador binario, así clasificando correctamente más datos en comparación con los otros dos casos. También se puede notar que el caso A entregó una mejor precisión que el caso B, y esto esto se debe al hecho de que en el caso A se entrenó el modelo usando solo los descriptores que tuvieron un mayor IV (Information Value). Al tener este mayor IV, son características más valiosas/importantes al momento de realizar la clasificación entre las clases y por tanto, se obtiene una mejor precisión en la clasificación en comparación con el caso B, en donde los descriptores usados no tienen un valor de IV tan alto en comparación con los descriptroes del caso A.\n",
    "\n",
    "- Mirando solo los tres casos entrenados con un modelo de **red neuronal** de dos capas y 20 neuronas por capa, se puede notar el mismo fenómeno que se observó con la regresión logística: la precisión es mayor cuando se usan solo los descriptores de mayor IV en comparación con el uso de descriptores con menor IV. También se puede notar que estas dos precisiones (casos A y B) fueron considerablemente mayores a la obtenida en los mismos casos de la regresión logísitca, por las razones expuestas en el ítem anterior. Nuevamente se puede observar que la precisión para el modelo de red neuronal entrenado usando todos los descriptores entregó la mejor precisión de todos los casos, y esto se puede deber nuevamente al hecho de que, teniendo en cuenta todos los descriptores del modelo se tiene un mejor criterio para el clasificador binario, así clasificando correctamente más datos en comparación con los otros dos casos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTE 3\n",
    "\n",
    "Los resultados obtenidos para las redes neuronales anteriores únicamente corresponden a una arquitectura. Un proceso necesario e importante en casos de estudio como este es la búsqueda de hiperparámetros, en este caso, el número de neuronas más adecuado (al menos dentro de cierto rango, este proceso también se conoce como [GridSearch](https://towardsdatascience.com/grid-search-for-model-tuning-3319b259367e)).\n",
    "\n",
    "**Divida el conjunto de datos de prueba en dos mitades: datos de validación y datos de prueba. Utilice los datos de validación para realizar una evaluación preliminar de cada modelo.** Utilice `train_test_split` para esta parte.\n",
    "\n",
    "Utilice todos los descriptores para esta prueba y realice las siguientes búsquedas:\n",
    "\n",
    "- Caso A: 1 capa escondida $\\times$ {5, 10, 20, 50, 100} neuronas.\n",
    "- Caso B: 2 capas escondidas $\\times$ {5, 10, 20, 50, 100} neuronas.\n",
    "- Caso C: 3 capas escondidas $\\times$ {5, 10, 20, 50, 100} neuronas.\n",
    "\n",
    "Utilice `matplotlib.pyplot` para graficar la precisión en los datos de validación, seleccione el mejor modelo, y obtenga una evaluación final para esta selección utilizando los datos de prueba.\n",
    "\n",
    "### Caso A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se parten los datos de prueba en dos subconjuntos: datos de validación y datos de prueba (versión 2).\n",
    "X_valid, X_test_2, y_valid, y_test_2 = train_test_split(X_test, y_test, \n",
    "                                                    stratify=y_test, test_size=0.5,\n",
    "                                                    random_state = 10086)\n",
    "\n",
    "# Se declaran los diferentes números de capas y neuronas que se van a validar.\n",
    "numCapas = [1, 2, 3]\n",
    "numNeus = [5, 10, 20, 50, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista para guardar las precisiones para el caso A.\n",
    "precisionesCasoA = []\n",
    "\n",
    "# # # # Implemente un ciclo FOR para entrenar cada una de las configuraciones solicitadas # # # #\n",
    "for neuronas in numNeus:\n",
    "    model = MLPClassifier(hidden_layer_sizes=(numCapas[0], neuronas), activation='relu', random_state=1) \n",
    "    model.fit(X_train, y_train) \n",
    "    y_predict = model.predict(X_valid) \n",
    "    prec = round(accuracy_score(y_valid, y_predict),5)\n",
    "    precisionesCasoA.append(prec) # Al entrenar cada configuración posible, se agrega la precisión del modelo a la lista.\n",
    "\n",
    "# Se grafican las precisiones para cada configuración solicitada.\n",
    "print(precisionesCasoA)\n",
    "plt.plot(numNeus, precisionesCasoA)\n",
    "plt.grid()\n",
    "plt.title('Precisiones para el caso A')\n",
    "plt.xlabel('# de neuronas')\n",
    "plt.ylabel('Precisión')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caso B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista para guardar las precisiones para el caso B.\n",
    "precisionesCasoB = []\n",
    "\n",
    "# # # # Implemente un ciclo FOR para entrenar cada una de las configuraciones solicitadas # # # #\n",
    "for neuronas in numNeus:\n",
    "    model = MLPClassifier(hidden_layer_sizes=(numCapas[1], neuronas), activation='relu', random_state=1) \n",
    "    model.fit(X_train, y_train) \n",
    "    y_predict = model.predict(X_valid) \n",
    "    prec = round(accuracy_score(y_valid, y_predict),5)\n",
    "    precisionesCasoB.append(prec) # Al entrenar cada configuración posible, se agrega la precisión del modelo a la lista.\n",
    "\n",
    "# Se grafican las precisiones para cada configuración solicitada.\n",
    "print(precisionesCasoB)\n",
    "plt.plot(numNeus, precisionesCasoB)\n",
    "plt.grid()\n",
    "plt.title('Precisiones para el caso B')\n",
    "plt.xlabel('# de neuronas')\n",
    "plt.ylabel('Precisión')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caso C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista para guardar las precisiones para el caso C.\n",
    "precisionesCasoC = []\n",
    "\n",
    "# # # # Implemente un ciclo FOR para entrenar cada una de las configuraciones solicitadas # # # #\n",
    "for neuronas in numNeus:\n",
    "    model = MLPClassifier(hidden_layer_sizes=(numCapas[2], neuronas), activation='relu', random_state=1) \n",
    "    model.fit(X_train, y_train) \n",
    "    y_predict = model.predict(X_valid) \n",
    "    prec = round(accuracy_score(y_valid, y_predict),5)\n",
    "    precisionesCasoC.append(prec) # Al entrenar cada configuración posible, se agrega la precisión del modelo a la lista.\n",
    "\n",
    "# Se grafican las precisiones para cada configuración solicitada.\n",
    "print(precisionesCasoC)\n",
    "plt.plot(numNeus, precisionesCasoC)\n",
    "plt.grid()\n",
    "plt.title('Precisiones para el caso C')\n",
    "plt.xlabel('# de neuronas')\n",
    "plt.ylabel('Precisión')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ahora se determina de forma automática cuál fue la configuración de red neuronal óptima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se genera una lista de las listas creadas previamente, para luego crear una sola lista con todas las precisiones.\n",
    "# Se sabe que los primeros 5 elementos de la lista corresponden al caso A, los siguientes 5 al caso B y los últimos 5\n",
    "# al caso C.\n",
    "lista = [precisionesCasoA, precisionesCasoB, precisionesCasoC]\n",
    "lista2 = list(itertools.chain.from_iterable(lista))\n",
    "\n",
    "# Se encuentra la precisión más alta con su respectivo índice dentro de la lista\n",
    "mejor = np.max(lista2)\n",
    "index_mejor = lista2.index(mejor)\n",
    "\n",
    "# Si el índice es menor o igual a 5, quiere decir que la mejor precisión ocurrió en el caso A\n",
    "# y por lo tanto, la cantidad de capas óptima es 1. Se guarda el número de capas y neuronas por\n",
    "# capa óptimas en variables correspondientes.\n",
    "if index_mejor <= 5:\n",
    "    capasOpt = numCapas[0]\n",
    "    indice_mejor = precisionesCasoA.index(mejor)\n",
    "    neuronas_opt = numNeus[indice_mejor]\n",
    "# Si el índice es mayor a 10, quiere decir que la mejor precisión ocurrió en el caso C\n",
    "# y por lo tanto, la cantidad de capas óptima es 3. Se guarda el número de capas y neuronas por\n",
    "# capa óptimas en variables correspondientes.\n",
    "elif index_mejor > 10:\n",
    "    capasOpt = numCapas[2]\n",
    "    indice_mejor = precisionesCasoC.index(mejor)\n",
    "    neuronas_opt = numNeus[indice_mejor]\n",
    "# Si el índice está entre 5 y 10, quiere decir que la mejor precisión ocurrió en el caso B\n",
    "# y por lo tanto, la cantidad de capas óptima es 2. Se guarda el número de capas y neuronas por\n",
    "# capa óptimas en variables correspondientes.\n",
    "else:\n",
    "    capasOpt = numCapas[1]\n",
    "    indice_mejor = precisionesCasoB.index(mejor)\n",
    "    neuronas_opt = numNeus[indice_mejor]\n",
    "\n",
    "# Se muestran los resultados óptimos.\n",
    "print(f'Número óptimo de capas: {capasOpt}')\n",
    "print(f'Número óptimo de neuronas por capa: {neuronas_opt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Se decidió escoger una arquitectura de red neuronal con 3 capas escondidas y 10 neuronas por capa porque, con los datos de validación, se obtuvo la mayor precisión usando esta arquitectura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráficas Evaluativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se entrena el modelo nuevamente, usando las cantidades óptimas de capas y número de neuronas por capa hallado previamente \n",
    "# y se prueban con los datos de prueba (versión 2).\n",
    "model = MLPClassifier(hidden_layer_sizes=(capasOpt, neuronas_opt), activation='relu', random_state=1) \n",
    "model.fit(X_train, y_train) \n",
    "y_predict = model.predict(X_test_2) \n",
    "prec = round(accuracy_score(y_test_2, y_predict),5)\n",
    "\n",
    "# Se muestra la precisión obtenida con los datos de prueba (versión 2).\n",
    "print(f'Precisión con parámetros óptimos ({capasOpt} capas escondidas, {neuronas_opt} neuronas por capa): {prec}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se muestra la matriz de confusión para el modelo probado con los datos de prueba (versión 2) y los parámetros óptimos.\n",
    "print(pd.DataFrame(confusion_matrix(y_test_2, y_predict)))\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix(y_test_2, y_predict),\n",
    "                      classes=class_names, \n",
    "                      title='Matriz de Confusión Normalizada: Red Neuronal con parámetros óptimos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Se comprobó con los datos de prueba que la elección de parámetros fue óptima, pues se obtuvo un valor de precisión alto y la cantidad de errores tipo 1 y tipo 0 es mucho menor comparada con todas las clasificaciones correctas que la red neuronal arrojó.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bono (2 puntos)\n",
    "\n",
    "Implemente el mejor modelo de red neuronal. Desarrolle el método de **_backpropagation_** para realizar el entrenamiento de la red sin utilizar ningún tipo de librería que tenga funciones prestablecidas con objetivos de apoyo en el tema de Machine Learning. Puede utilizar Numpy, Pandas, etc. NO puede utilizar: SciKit-Learn, Tensorflow/Keras, PyTorch, etc.\n",
    "\n",
    "El bono debe estar COMPLETO y se debe observar una curva de aprendizaje a través de las iteraciones que permita obtener resultados aceptables con respecto a la red definida a partir de SciKit-Learn. De lo contrario, no se tomará como válido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # Implemente el método de backpropagation para la arquitectura seleccionada # # # #\n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# ... "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "589b85e89a6335d0b08d77c78c1928c4f41a1a43a63e50b0e56ffdda28f0c24f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
